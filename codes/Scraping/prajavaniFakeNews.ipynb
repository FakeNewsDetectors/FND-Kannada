{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"prajavaniFakeNews.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["##Importing libraries"],"metadata":{"id":"x-xTsV4lhrQe"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"H5l7_lXWIGS4"},"outputs":[],"source":["import requests\n","from bs4 import BeautifulSoup\n","import csv\n","import pandas as pd"]},{"cell_type":"markdown","source":["##Scraping prajavani.net"],"metadata":{"id":"7_TdKAyzh1yu"}},{"cell_type":"code","source":["URL = \"https://www.prajavani.net/factcheck\"\n","r = requests.get(URL) \n","soup = BeautifulSoup(r.content, 'html5lib')\n","\n","soup.prettify()"],"metadata":{"id":"0-9e1U9dL1TR"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Extracting links"],"metadata":{"id":"RjyaVEu8h7Ps"}},{"cell_type":"code","source":["lists = []\n","final_list = []\n","\n","table = soup.find_all('div', attrs = {'class':'pj-top-thump'})\n","for element in table:\n","  lists.append(element.find('a'))\n","\n","#formatting\n","for link in lists:\n","  url = str(link)\n","  url = url.split('\"')[1]\n","  temp_list=[]\n","  temp_list.append(url)\n","  final_list.append(temp_list)\n","\n","print(final_list)"],"metadata":{"id":"dkV1SDaAMq0I"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Checkpoint: CSV file containing links generated"],"metadata":{"id":"SXM1sGHmibmN"}},{"cell_type":"code","source":["with open('prajavaniLinks.csv', 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerows(final_list)"],"metadata":{"id":"gMhzaY6uNnJZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Reading csv file with links of all fake news articles"],"metadata":{"id":"abw4avq9ileJ"}},{"cell_type":"code","source":["df = pd.read_csv('prajavaniLinks.csv')\n","df.columns = ['url']\n","df['url'] = 'https://www.prajavani.net' + df['url']"],"metadata":{"id":"Y-N3tc5WPpnT"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Scraping fake news headlines"],"metadata":{"id":"KFh5UQC3ivBl"}},{"cell_type":"code","source":["fake_news = []\n","for i in range(len(df)):\n","  URL = df.loc[i, 'url']\n","  r = requests.get(URL) \n","  soup = BeautifulSoup(r.content, 'html5lib')\n","  table = soup.find('h1', attrs = {'id' : 'page-title'})\n","  fake_news.append(table)"],"metadata":{"id":"PPMbsvB1P9NU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Extracting headlines between h1 tags"],"metadata":{"id":"_bxj2o5Ei3tT"}},{"cell_type":"code","source":["final_list=[]\n","\n","for i in fake_news:\n","  for j in i:\n","    temp_list = []\n","    temp_headline = str(j)\n","    temp_headline = temp_headline.split(\"k\")\n","    if len(temp_headline)==1:\n","      temp_list.append(temp_headline[0])\n","    else:\n","      temp_list.append(temp_headline[1])\n","    final_list.append(temp_list)\n","\n","print(final_list)\n","print(\"Length: \", len(final_list))"],"metadata":{"id":"o3VE1eiuQAjh"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["##Final output: CSV file containing fake news headlines generated"],"metadata":{"id":"5m_O6iESjH68"}},{"cell_type":"code","source":["with open('prajavaniFakeNews.csv', 'w', newline='') as f:\n","    writer = csv.writer(f)\n","    writer.writerows(final_list)"],"metadata":{"id":"z35_bqg9QXE8"},"execution_count":null,"outputs":[]}]}